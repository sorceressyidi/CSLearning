{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CART Algorithm (Classification and Regression Trees)\n",
    "\n",
    "Reference :\n",
    "\n",
    "https://blog.csdn.net/weixin_45666566/article/details/107954454\n",
    "\n",
    "https://zhuanlan.zhihu.com/p/461032990"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cart algorithm is a simple algorithm that uses a decision tree to predict the target variable based on several input variables.\n",
    "\n",
    "First we implement the necessary packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from math import log\n",
    "import operator\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Splitting Creteria\n",
    "* For classification tasks, CART uses Gini impurity as the splitting criterion. The lower the Gini impurity, the more pure the subset is. \n",
    "\n",
    "* For regression tasks, CART uses residual reduction as the splitting criterion. The lower the residual reduction, the better the fit of the model to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regression Tree\n",
    "Here, we try to implement the algorithm used in the regression tree.\n",
    "\n",
    "* Reference : https://github.com/datawhalechina/statistical-learning-method-solutions-manual/blob/master/docs/chapter05/ch05.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, value, feature, left=None, right=None):\n",
    "        \"\"\"\n",
    "        Node class represents a node in the regression tree.\n",
    "        \n",
    "        Args:\n",
    "            value (list): The value of the node.\n",
    "            feature (list): The feature of the node.\n",
    "            left (Node): The left child node.\n",
    "            right (Node): The right child node.\n",
    "        \"\"\"\n",
    "        self.value = value.tolist()\n",
    "        self.feature = feature.tolist()\n",
    "        self.left = left\n",
    "        self.right = right\n",
    "\n",
    "class MyLeastSquareRegTree:\n",
    "    def __init__(self, train_X, y, epsilon):\n",
    "        \"\"\"\n",
    "        MyLeastSquareRegTree class represents a Least Square Regression Tree.\n",
    "\n",
    "        Args:\n",
    "            train_X (numpy.ndarray): The training features.\n",
    "            y (numpy.ndarray): The target values.\n",
    "            epsilon (float): The threshold for stopping tree growth.\n",
    "        \"\"\"\n",
    "        self.x = train_X\n",
    "        self.y = y\n",
    "        self.epsilon = epsilon\n",
    "        self.feature_count = len(train_X[0])\n",
    "        self.tree = None\n",
    "\n",
    "    def _fit(self, x, y, feature_count):\n",
    "        \"\"\"\n",
    "        Fits the regression tree recursively.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): The features.\n",
    "            y (numpy.ndarray): The target values.\n",
    "            feature_count (int): The number of features.\n",
    "\n",
    "        Returns:\n",
    "            Node: The root node of the fitted tree.\n",
    "        \"\"\"\n",
    "        (j, s, minval, c1, c2) = self._divide(x, y, feature_count)\n",
    "        tree = Node(feature=j, value=x[s, j], left=None, right=None)\n",
    "\n",
    "        if minval < self.epsilon or len(y[np.where(x[:, j] <= x[s, j])]) <= 1:\n",
    "            tree.left = c1\n",
    "        else:\n",
    "            tree.left = self._fit(x[np.where(x[:, j] <= x[s, j])],\n",
    "                                  y[np.where(x[:, j] <= x[s, j])],\n",
    "                                  self.feature_count)\n",
    "        if minval < self.epsilon or len(y[np.where(x[:, j] > s)]) <= 1:\n",
    "            tree.right = c2\n",
    "        else:\n",
    "            tree.right = self._fit(x[np.where(x[:, j] > x[s, j])],\n",
    "                                   y[np.where(x[:, j] > x[s, j])],\n",
    "                                   self.feature_count)\n",
    "        return tree\n",
    "\n",
    "    def fit(self):\n",
    "        \"\"\"\n",
    "        Fits the regression tree to the training data.\n",
    "        \"\"\"\n",
    "        self.tree = self._fit(self.x, self.y, self.feature_count)\n",
    "        return self\n",
    "    \n",
    "\n",
    "    def printtree(self, node=None, indent='', last=True):\n",
    "        \"\"\"\n",
    "        Prints the regression tree in a tree-like structure.\n",
    "\n",
    "        Args:\n",
    "            node (Node): The current node.\n",
    "            indent (str): The indentation for pretty printing.\n",
    "            last (bool): Flag indicating if the current node is the last child.\n",
    "        \"\"\"\n",
    "        if node is None:\n",
    "            node = self.tree\n",
    "\n",
    "        if isinstance(node, Node):\n",
    "            print(indent, end='')\n",
    "            if last:\n",
    "                print(\"└─ \", end='')\n",
    "                indent += \"   \"\n",
    "            else:\n",
    "                print(\"├─ \", end='')\n",
    "                indent += \"│  \"\n",
    "            print(f\"Feature: {node.feature}, Value: {node.value}\")\n",
    "            if node.left is not None or node.right is not None:\n",
    "                self.printtree(node.left, indent, last=False)\n",
    "                self.printtree(node.right, indent, last=True)\n",
    "        else:\n",
    "            print(indent, end='')\n",
    "            if last:\n",
    "                print(\"└─ \", end='')\n",
    "            else:\n",
    "                print(\"├─ \", end='')\n",
    "            print(f\"Prediction: {node}\")\n",
    "            \n",
    "\n",
    "    @staticmethod\n",
    "    def _divide(x, y, feature_count):\n",
    "        \"\"\"\n",
    "        Finds the best split for the given data.\n",
    "\n",
    "        Args:\n",
    "            x (numpy.ndarray): The features.\n",
    "            y (numpy.ndarray): The target values.\n",
    "            feature_count (int): The number of features.\n",
    "\n",
    "        Returns:\n",
    "            tuple: The best split information (feature index, split index, minimum value, left child mean, right child mean).\n",
    "        \"\"\"\n",
    "        \n",
    "        cost = np.zeros((feature_count, len(x)))\n",
    "        for i in range(feature_count):\n",
    "            for k in range(len(x)):\n",
    "                value = x[k, i]\n",
    "                y1 = y[np.where(x[:, i] <= value)]\n",
    "                c1 = np.mean(y1)\n",
    "                y2 = y[np.where(x[:, i] > value)]\n",
    "                if len(y2) == 0:\n",
    "                    c2 = 0\n",
    "                else:\n",
    "                    c2 = np.mean(y2)\n",
    "                \n",
    "                y1[:] = y1[:] - c1\n",
    "                y2[:] = y2[:] - c2\n",
    "                cost[i, k] = np.sum(y1 * y1) + np.sum(y2 * y2)\n",
    "        \n",
    "        cost_index = np.where(cost == np.min(cost))\n",
    "        j = cost_index[0][0]\n",
    "        s = cost_index[1][0]\n",
    "        \n",
    "        c1 = np.mean(y[np.where(x[:, j] <= x[s, j])])\n",
    "        c2 = np.mean(y[np.where(x[:, j] > x[s, j])])\n",
    "        \n",
    "        return j, s, cost[cost_index], c1, c2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Test the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "└─ Feature: 0, Value: 5\n",
      "   ├─ Feature: 0, Value: 3\n",
      "   │  ├─ Prediction: 4.72\n",
      "   │  └─ Prediction: 5.57\n",
      "   └─ Feature: 0, Value: 7\n",
      "      ├─ Feature: 0, Value: 6\n",
      "      │  ├─ Prediction: 7.05\n",
      "      │  └─ Prediction: 7.9\n",
      "      └─ Feature: 0, Value: 8\n",
      "         ├─ Prediction: 8.23\n",
      "         └─ Prediction: 8.85\n"
     ]
    }
   ],
   "source": [
    "train_X = np.array([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]]).T\n",
    "y = np.array([4.50, 4.75, 4.91, 5.34, 5.80, 7.05, 7.90, 8.23, 8.70, 9.00])\n",
    "\n",
    "model_tree = MyLeastSquareRegTree(train_X, y, epsilon=0.2)\n",
    "model_tree.fit()\n",
    "model_tree.printtree()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification Tree\n",
    "Here, we try to implement the algorithm used in the regression tree.\n",
    "\n",
    "This Section mainly contains :\n",
    "\n",
    "* The implementation of basic data structures and initialization\n",
    "\n",
    "* Dini \n",
    "\n",
    "* Splitting\n",
    "\n",
    "* The final output of a classification tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Initialize the dataset\n",
    "def createDataSet():\n",
    "    \"\"\"\n",
    "    Create example data / Read data\n",
    "    @param dataSet: The dataset\n",
    "    @return dataSet labels: The dataset, Feature set\n",
    "    \"\"\"\n",
    "    dataSet = [('青年', '否', '否', '一般', '不同意'),\n",
    "               ('青年', '否', '否', '好', '不同意'),\n",
    "               ('青年', '是', '否', '好', '同意'),\n",
    "               ('青年', '是', '是', '一般', '同意'),\n",
    "               ('青年', '否', '否', '一般', '不同意'),\n",
    "               ('中年', '否', '否', '一般', '不同意'),\n",
    "               ('中年', '否', '否', '好', '不同意'),\n",
    "               ('中年', '是', '是', '好', '同意'),\n",
    "               ('中年', '否', '是', '非常好', '同意'),\n",
    "               ('中年', '否', '是', '非常好', '同意'),\n",
    "               ('老年', '否', '是', '非常好', '同意'),\n",
    "               ('老年', '否', '是', '好', '同意'),\n",
    "               ('老年', '是', '否', '好', '同意'),\n",
    "               ('老年', '是', '否', '非常好', '同意'),\n",
    "               ('老年', '否', '否', '一般', '不同意')]\n",
    "    # 特征集\n",
    "    labels = ['年龄', '有工作', '有房子', '信贷情况']\n",
    "    return dataSet,labels\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get a closer look at the dataset and its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('青年', '否', '否', '一般', '不同意')\n",
      "---------------\n",
      "{'中年', '青年', '老年'}\n",
      "{'是', '否'}\n",
      "{'是', '否'}\n",
      "{'非常好', '一般', '好'}\n",
      "{'不同意', '同意'}\n"
     ]
    }
   ],
   "source": [
    "dataset,labels = createDataSet()\n",
    "print(dataset[0])\n",
    "print(\"---------------\")\n",
    "for i in range(len(dataset[0])):\n",
    "    unique_values = set(data[i] for data in dataset) # Remove duplicates\n",
    "    print(unique_values)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We assume the \"first feature\" (p) is \"同意\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = '同意'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a function to effectively split the dataset according to the given features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('青年', '否', '否', '一般', '不同意')\n",
      "('青年', '否', '否', '好', '不同意')\n",
      "('青年', '是', '否', '好', '同意')\n",
      "('青年', '是', '是', '一般', '同意')\n",
      "('青年', '否', '否', '一般', '不同意')\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data_set, index, value):\n",
    "    \"\"\"\n",
    "    Split the dataset based on a specific feature value.\n",
    "    @param data_set: The dataset\n",
    "    @param index: Index of the feature column\n",
    "    @param value: Value of the feature to split on\n",
    "    @return subset: Subset of the dataset with the specified feature value\n",
    "    \"\"\"\n",
    "    subset = [data for data in data_set if data[index] == value]\n",
    "    return subset\n",
    "\n",
    "### Test the split_dataset function\n",
    "subset = split_dataset(dataset, 0, '青年')\n",
    "for i in range(len(subset)):\n",
    "    print(subset[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, use a simple function to calculate the probability 'p'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4 0.6\n"
     ]
    }
   ],
   "source": [
    "def calculate_probability(class_list, feature):\n",
    "    \"\"\"\n",
    "    Calculate the probability that a sample point belongs to a specific class.\n",
    "    @param class_list: List of class labels\n",
    "    @param feature: Feature to calculate the probability for\n",
    "    @return probability: Probability of the feature\n",
    "    \"\"\"\n",
    "    total_samples = len(class_list)\n",
    "    feature_count = np.sum(np.array(class_list) == feature)\n",
    "    return feature_count / total_samples\n",
    "\n",
    "### Test the calculate_probability function\n",
    "probability1 = calculate_probability(subset, '同意')\n",
    "probability2 = calculate_probability(subset, '不同意')\n",
    "print(probability1, probability2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now , we implement to core part of the aigorithm : choose the split to minimize Dini index.\n",
    "> Note: Gini index of the probability distribution for binary classification problem is $Gini(p)=2p(1-p)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "有房子\n"
     ]
    }
   ],
   "source": [
    "def choose_best_feature_to_split(data_set):\n",
    "    \"\"\"\n",
    "    Choose the best feature for splitting the dataset.\n",
    "    @param data_set: The dataset\n",
    "    @return best_feature: Index of the best feature\n",
    "    \"\"\"\n",
    "    num_features = len(data_set[0]) - 1\n",
    "    # If there is only one feature, return 0\n",
    "    if num_features == 1:\n",
    "        return 0\n",
    "\n",
    "    best_gini = 1\n",
    "    best_feature = -1\n",
    "    for i in range(num_features):\n",
    "        # Get the unique values of the feature\n",
    "        unique_values = set(data[i] for data in data_set) # Remove duplicates\n",
    "        feature_gini = 0  # Gini index for the feature\n",
    "        \n",
    "        # Calculate the Gini index for the feature\n",
    "        for value in unique_values:\n",
    "            subset = split_dataset(data_set, i, value) # Split the dataset\n",
    "            prob = len(subset) / len(data_set)\n",
    "            p = calculate_probability([data[-1] for data in subset], feature)\n",
    "            feature_gini += prob * (2 * p * (1 - p))\n",
    "        # Choose the feature with the lowest Gini index\n",
    "        if feature_gini < best_gini:\n",
    "            best_gini = feature_gini\n",
    "            best_feature = i\n",
    "\n",
    "    return best_feature\n",
    "# Test the choose_best_feature_to_split function\n",
    "best_feature = choose_best_feature_to_split(dataset)\n",
    "print(labels[best_feature])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember? When the algorithm meets the termination conditions but encounters leaf nodes with multiple classes, we determine the majority class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "同意\n"
     ]
    }
   ],
   "source": [
    "def majorityCnt(classList):\n",
    "    \"\"\"\n",
    "    Determines the most frequent class in a list.\n",
    "    \n",
    "    @param classList: A list containing class labels.\n",
    "    @return majorityClass: The most frequent class label in the list.\n",
    "    \"\"\"\n",
    "    class_count = {}  # Dictionary to store the count of each class\n",
    "    for label in classList:\n",
    "        class_count[label] = class_count.get(label, 0) + 1  # Count the occurrences of each class\n",
    "    # Sort the class counts in descending order and return the most frequent class\n",
    "    majorityClass = max(class_count, key=class_count.get)\n",
    "    return majorityClass\n",
    "# Test the majorityCnt function\n",
    "classList = [data[-1] for data in dataset]\n",
    "majority_class = majorityCnt(classList)\n",
    "print(majority_class)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can complete the whole algorithm and create the classifictaion tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createTree(dataSet, labels):\n",
    "    \"\"\"\n",
    "    Classify the last feature, sort by the number of classes after classification, e.g., if the final classification is 2 agree and 1 disagree, it is determined as agree.\n",
    "    @param dataSet: The dataset\n",
    "    @param labels: The feature set\n",
    "    @return myTree: The decision tree\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get the last value of each row, i.e., the class of each row\n",
    "    classList = [example[-1] for example in dataSet]  \n",
    "    \n",
    "    # When the dataset has only one class\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return classList[0]\n",
    "    \n",
    "    # When there is only one column left in the dataset (i.e., the class), classify based on the last feature\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return majorityCnt(classList)\n",
    "    \n",
    "    # Other cases\n",
    "    bestFeat = choose_best_feature_to_split(dataSet)  # Choose the best feature (column)\n",
    "    bestFeatLabel = labels[bestFeat]  # The best feature\n",
    "    del(labels[bestFeat])  # Remove the current best feature from the feature set\n",
    "    uniqueVals = set(example[bestFeat] for example in dataSet)  # Select the unique values corresponding to the best feature\n",
    "    myTree = {bestFeatLabel: {}}  # Save the classification result as a dictionary\n",
    "    \n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]  # Deep copy, the copied value is independent of the original value (normal copying is shallow copying, changes to the original value or the copied value affect each other)\n",
    "        myTree[bestFeatLabel][value] = createTree(split_dataset(dataSet, bestFeat, value), subLabels)  # Recursively call to create the decision tree\n",
    "        \n",
    "    return myTree\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'有房子': {'是': '同意', '否': {'有工作': {'是': '同意', '否': '不同意'}}}}\n"
     ]
    }
   ],
   "source": [
    "dataSet, labels = createDataSet()  \n",
    "print(createTree(dataSet, labels)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can better represent our tree using more clear **TREE** structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Feature: 有房子\n",
      "   Value: 是\n",
      "     Prediction: 同意\n",
      "   Value: 否\n",
      "     Feature: 有工作\n",
      "       Value: 是\n",
      "         Prediction: 同意\n",
      "       Value: 否\n",
      "         Prediction: 不同意\n"
     ]
    }
   ],
   "source": [
    "class TreeNode:\n",
    "    def __init__(self, feature=None, value=None, prediction=None):\n",
    "        self.feature = feature          # Feature name\n",
    "        self.value = value              # Feature value\n",
    "        self.prediction = prediction    # If it's a leaf node, prediction value; otherwise, None\n",
    "        self.children = {}              # Children nodes, stored as {feature value: child node}\n",
    "\n",
    "def createTree(dataSet, labels):\n",
    "    \"\"\"\n",
    "    Create a decision tree based on the dataset and labels.\n",
    "    @param dataSet: The dataset\n",
    "    @param labels: The feature labels\n",
    "    @return myTree: The root node of the decision tree\n",
    "    \"\"\"\n",
    "    classList = [example[-1] for example in dataSet]\n",
    "\n",
    "    # When all samples belong to the same class, return that class as the prediction of the leaf node\n",
    "    if classList.count(classList[0]) == len(classList):\n",
    "        return TreeNode(prediction=classList[0])\n",
    "    # When there's only one feature left in the dataset, choose the majority class as the prediction of the leaf node\n",
    "    if len(dataSet[0]) == 1:\n",
    "        return TreeNode(prediction=majorityCnt(classList))\n",
    "\n",
    "    # Choose the best splitting feature\n",
    "    bestFeat = choose_best_feature_to_split(dataSet)\n",
    "    bestFeatLabel = labels[bestFeat]\n",
    "\n",
    "    # Create the current node\n",
    "    currentNode = TreeNode(feature=bestFeatLabel)\n",
    "\n",
    "    # For each value of the current feature, recursively build the subtree\n",
    "    uniqueVals = set(example[bestFeat] for example in dataSet)\n",
    "    for value in uniqueVals:\n",
    "        subLabels = labels[:]  # Create a copy of labels\n",
    "        currentNode.children[value] = createTree(split_dataset(dataSet, bestFeat, value), subLabels)\n",
    "    return currentNode\n",
    "\n",
    "def printTree(node, depth=0):\n",
    "    \"\"\"\n",
    "    Print the decision tree in a human-readable format.\n",
    "    @param node: The root node of the decision tree\n",
    "    @param depth: The depth of the current node in the tree\n",
    "    \"\"\"\n",
    "    if node is None:\n",
    "        return\n",
    "    if node.prediction is not None:\n",
    "        print(\"  \" * depth, \"Prediction:\", node.prediction)\n",
    "    else:\n",
    "        print(\"  \" * depth, \"Feature:\", node.feature)\n",
    "        for value, child in node.children.items():\n",
    "            print(\"  \" * (depth + 1), f\"Value: {value}\")\n",
    "            printTree(child, depth + 2)\n",
    "\n",
    "# Example usage:\n",
    "dataSet, labels = createDataSet()\n",
    "myTree = createTree(dataSet, labels)\n",
    "printTree(myTree)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try our classification tree now!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "不同意\n",
      "同意\n"
     ]
    }
   ],
   "source": [
    "def classify(inputTree, featLabels, testVec):\n",
    "    \"\"\"\n",
    "    Classify the input vector based on the decision tree.\n",
    "    @param inputTree: The decision tree\n",
    "    @param featLabels: The feature set\n",
    "    @param testVec: The input vector\n",
    "    @return classLabel: The class label\n",
    "    \"\"\"\n",
    "    currentNode = inputTree\n",
    "    while currentNode.children:\n",
    "        feature = currentNode.feature\n",
    "        featIndex = featLabels.index(feature)\n",
    "        value = testVec[featIndex]\n",
    "        if value in currentNode.children:\n",
    "            currentNode = currentNode.children[value]\n",
    "        else: # If the value is not found in the children, return None (or handle appropriately)\n",
    "            return None\n",
    "    return currentNode.prediction\n",
    "\n",
    "\n",
    "print(classify(myTree, ['年龄', '有工作', '有房子', '信贷情况'], ['青年', '否', '否', '一般']))\n",
    "print(classify(myTree, ['年龄', '有工作', '有房子', '信贷情况'], ['老年', '是', '否', '好']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cut Branches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need a function to calculate the loss\n",
    "\n",
    "* Normaly we use the cross validation,here for simplicity, we just validate on the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n"
     ]
    }
   ],
   "source": [
    "def calculate_error(tree, testData, labels):\n",
    "    \"\"\"\n",
    "    Calculates the error of the decision tree on the test data.\n",
    "    \n",
    "    @param tree: The decision tree\n",
    "    @param testData: Test data to evaluate the decision tree\n",
    "    @param labels: The feature set\n",
    "    @return error: The error rate of the decision tree on the test data\n",
    "    \"\"\"\n",
    "    num_samples = len(testData)\n",
    "    num_errors = 0\n",
    "    for data in testData:\n",
    "        pred = classify(tree, labels, data[:-1])  # Get the prediction from the decision tree\n",
    "        if pred != data[-1]:\n",
    "            num_errors += 1\n",
    "    \n",
    "    error_rate = num_errors / num_samples\n",
    "    return float(error_rate)\n",
    "\n",
    "# Test the calculate_error function\n",
    "\n",
    "dataSet, labels = createDataSet() \n",
    "myTree = createTree(dataSet, labels)\n",
    "#note we have deleted some feature coloumns,we have to keep record of the deleted feature columns\n",
    "\n",
    "testData = [('青年', '否', '否', '一般', '同意'),\n",
    "            ('老年', '是', '否', '好', '同意')]\n",
    "\n",
    "print(calculate_error(myTree, testData,labels))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data = load_iris()\n",
    "x, y = data.data, data.target\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "model = DecisionTreeClassifier(criterion='gini')\n",
    "model.fit(x_train, y_train)\n",
    "print(model.score(x_test, y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
